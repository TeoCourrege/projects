{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFmaQ9uuYbDP"
      },
      "source": [
        "# Lab: Transformer in Practice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etCgF-aAiz_H"
      },
      "source": [
        "We train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\n",
        "\n",
        "A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the rest of the lab for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "[``nn.TransformerEncoderLayer``](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer).\n",
        "\n",
        "Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked.\n",
        "\n",
        "To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy2R23Wciz-_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4esKuC8YN5Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oveLu8vjsjiL"
      },
      "outputs": [],
      "source": [
        "#Colab\n",
        "#!pip install torchtext\n",
        "#conda\n",
        "#conda install -c pytorch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adlOFAk4tCfW",
        "outputId": "947ba167-5ceb-4751-cff0-724872741e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8.0a0+0f911ec\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEy4q9Oiz_H"
      },
      "source": [
        "## Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLycnGodQl5z"
      },
      "source": [
        "#### First steps with the transformer layers\n",
        "\n",
        "\"TransformerEncoderLayer\" is made up of self-attn and feedforward network. This standard encoder layer is based on the paper “Attention Is All You Need”.\n",
        "\n",
        "\"dim_feedforward\" is the dimension of the hidden layer in the feedforward neural network.\n",
        "\n",
        "More details on them [here](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWDnJiWNQl5z",
        "outputId": "70ea310b-95bc-4cf5-b96c-5db3cd6dbd20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 32, 512])\n",
            "torch.Size([10, 32, 512])\n"
          ]
        }
      ],
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=1000)\n",
        "src = torch.rand(10, 32, 512)\n",
        "out = encoder_layer(src)\n",
        "print(src.shape)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNHXBC0sQl50"
      },
      "source": [
        "We create a transformer as a stack of encoder layers. \"TransformerEncoder\" is a stack of \"num_layers\" encoder layers.\n",
        "\n",
        "More details on them [here](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_er-RtQl50",
        "outputId": "6a1798f4-fb60-44b6-efbe-4e206a31176a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 32, 512])\n",
            "torch.Size([10, 32, 512])\n"
          ]
        }
      ],
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "src = torch.rand(10, 32, 512)\n",
        "out = transformer_encoder(src)\n",
        "print(src.shape)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMDzDes6Ql50"
      },
      "source": [
        "#### __init__ parameters:\n",
        "- ntoken: the size of vocabulary (number of different tokens),\n",
        "- ninp: common dimension of each element of the input sequence,\n",
        "- nhead: number of heads in the multi-head attention,\n",
        "- nhid: number of neurons in the feedforward neural network,\n",
        "- nlayers: number of encoder layers in the tranformer's encoder,\n",
        "- dropout: probability of dropout\n",
        "\n",
        "___________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9wUp47KQl50"
      },
      "source": [
        "### Question: explain how the \"forward\" function works? What is the role of the mask \"src_mask\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOB_pJ_wQl51"
      },
      "source": [
        "- the input element (src) is embedded as a numeric vector (\"encoder\") with a scaling factor (squareroot of its dimension)\n",
        "- the embedded element is jointly encoded with its position (\"pos_encoder\")\n",
        "- the input element is processed by the transformer's encoder (\"transformer_encoder\"). It is a TransformerEncoder with a stack of N encoder layers\n",
        "- the outout is produced by the decoder (\"decoder\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvGmcEwgiz_I"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    # Generate an upper triangular mask: lower part is -inf and upper part is 0.0\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        # torch.triu returns a copy of a matrix with the elements below the k-th diagonal zeroed.\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src) # add the positional encoding (pe) to src: src <- src + pe\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwmST9ykiz_I"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy7KGeJeQl52"
      },
      "source": [
        "#### the class __PositionalEncoding__ adds the sine or cosine components to the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjnutiJBiz_I"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2HM5oWrQl52"
      },
      "source": [
        "#### Response:\n",
        "\n",
        "The mask __scr_mask__ prevents the encoder to use the information in the future. Hence, the prediction is just based on the past information.\n",
        "\n",
        "This masking, combined with fact that the targets are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
        "\n",
        "Technically, if a FloatTensor is provided as the mask, it will be added to the attention weight.\n",
        "\n",
        "See details [here](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1867PGWQl52",
        "outputId": "2426f9d3-863b-4502-b424-0e7287acb982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "# Test the function to generate a mask\n",
        "tmp_model = TransformerModel(10, 4, 4, 4, 2, 0.5) # just a model to study the mak. The parameters of the model have no meaning.\n",
        "src_mask_dim5 = tmp_model.generate_square_subsequent_mask(5)\n",
        "print(src_mask_dim5)\n",
        "\n",
        "print(src_mask_dim5.type())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poVgMGKQQl52",
        "outputId": "63eb6682-d544-4850-ad93-2d6fda7ec3f4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Print the mask as an image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(src_mask_dim5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH5mfkj1iz_J"
      },
      "source": [
        "_____________________________________________________________________________\n",
        "\n",
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTA_RjUGiz_J"
      },
      "source": [
        "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7vnHiyGQl53"
      },
      "source": [
        "#### What is TorchText?\n",
        "\n",
        "TorchText is a pytorch package that contains different data processing methods as well as popular NLP datasets. According to the official PyTorch documentation, torchtext has 4 main functionalities: data, datasets, vocab, and utils. Data is mainly used to create custom dataset class, batching samples etc. Datasets consists of the various NLP datasets from sentiment analysis to question answering. Vocab covers different methods of processing text and utils consists of additional helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZrwYnrc6ec"
      },
      "source": [
        "#### Warning: path to access the dataset\n",
        "\n",
        "The following variables \"from_path\" and \"to_path\" are necessary to store the dataset.\n",
        "\n",
        "They are defined for Google colab.\n",
        "\n",
        "You must change them if you are using your personal Python installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyaOM6k7c4sX"
      },
      "outputs": [],
      "source": [
        "# Google Colab\n",
        "#from_path = '/content/sample_data/wikitext-2-v1.zip'\n",
        "#to_path = '/content/sample_data/'\n",
        "\n",
        "# Personal laptop\n",
        "from_path = './wikitext-2-v1.zip'\n",
        "to_path = './'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUd0qpUOdSCE"
      },
      "source": [
        "Download and preprocess the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDD3ZMPixyw9"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import torch\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
        "download_from_url(url, from_path)\n",
        "test_filepath, valid_filepath, train_filepath = extract_archive(from_path, to_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o96_GiPJQl54"
      },
      "source": [
        "Print the file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSEZ3o1VkixN",
        "outputId": "04d05af0-63db-4740-d38c-86e0be62a412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./wikitext-2/wiki.test.tokens\n",
            "./wikitext-2/wiki.valid.tokens\n",
            "./wikitext-2/wiki.train.tokens\n"
          ]
        }
      ],
      "source": [
        "print(test_filepath)\n",
        "print(valid_filepath)\n",
        "print(train_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_cwaqPmC5T"
      },
      "source": [
        "### Question: what is the role of \"tokenizer\".\n",
        "\n",
        "The function get_tokenizer(\"basic_english\") generate a tokenizer function for a string sentence. It is dedicated to english language.\n",
        "\n",
        "A tokenizer deels with the [tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/). Tokenization is a way of separating a piece of text into smaller units called tokens.\n",
        "\n",
        "The following cell gives an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7xkWsHKloJs",
        "outputId": "83e59648-e37a-4f9c-da64-05287d51604f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6g_HMZQl55"
      },
      "source": [
        "Just see a part of the raw text to understand how the text will be preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpRPaxN-oflQ",
        "outputId": "0b7efebe-76c3-4024-b31d-f7866cc627c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Item 0\n",
            " \n",
            "\n",
            " \n",
            "<class 'str'>\n",
            "[]\n",
            "Item 1\n",
            " = Homarus gammarus = \n",
            "\n",
            " = Homarus gammarus = \n",
            "<class 'str'>\n",
            "['=', 'homarus', 'gammarus', '=']\n",
            "Item 2\n",
            " \n",
            "\n",
            " \n",
            "<class 'str'>\n",
            "[]\n",
            "Item 3\n",
            " Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \n",
            "\n",
            " Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \n",
            "<class 'str'>\n",
            "['homarus', 'gammarus', ',', 'known', 'as', 'the', 'european', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'atlantic', 'ocean', ',', 'mediterranean', 'sea', 'and', 'parts', 'of', 'the', 'black', 'sea', '.', 'it', 'is', 'closely', 'related', 'to', 'the', 'american', 'lobster', ',', 'h', '.', 'americanus', '.', 'it', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'in', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', 'lobster', 'red', 'on', 'cooking', '.', 'mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'british', 'isles', '.']\n"
          ]
        }
      ],
      "source": [
        "raw_text_iter = iter(io.open(valid_filepath, encoding=\"utf8\"))\n",
        "\n",
        "#Line breaks in text are generally represented as:\n",
        "#\n",
        "#\\r\\n - on a windows computer\n",
        "#\\r - on an Apple computer\n",
        "#\\n - on Linux\n",
        "\n",
        "for number in range(4):\n",
        "    print(\"Item \" + str(number))\n",
        "    item = next(raw_text_iter)\n",
        "    print(item)\n",
        "    item = item.replace(\"\\n\",\"\")\n",
        "    print(item)\n",
        "    print(type(item))\n",
        "    print(tokenizer(item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prkkn3OMQl55"
      },
      "source": [
        "### Question: what is the role of \"vocab\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8_QHVreQl55",
        "outputId": "79e72540-ec67-41fb-aa0b-fc9d8def01ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "36718lines [00:01, 24055.30lines/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer,\n",
        "                                      iter(io.open(train_filepath,\n",
        "                                                   encoding=\"utf8\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrwqIITQl56"
      },
      "source": [
        "It defines a vocabulary object that will be used to numericalize a field.\n",
        "\n",
        "\"vocab\" is basically a dictionary (the dictionary is the field \"stoi\")\n",
        "The dictionary encodes a word as a number.\n",
        "There are two special characters:\n",
        "\n",
        "pad_token – The string token used as padding. Default: “<pad>”.\n",
        "    \n",
        "unk_token – The string token used to represent OOV words. Default: “<unk>”.\n",
        "    \n",
        "Padding can be useful when we want to process some sentences with a constant length. The pad symbols is added to adjust the sentence length.\n",
        "    \n",
        "The following cell illustrates the behavior of \"vocab\" with a given itemized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtoDd2FtQl56",
        "outputId": "c3e965f7-94ac-472b-d46c-9b6efbdd6baf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3lines [00:00, 10313.86lines/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'european', 'known', 'the']\n",
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7ff94c167b80>>, {'<unk>': 0, '<pad>': 1, 'european': 2, 'known': 3, 'the': 4})\n",
            "[0, 1, 2, 3, 4]\n",
            "0\n",
            "1\n",
            "0\n",
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mytext = ['known', 'the', 'european']\n",
        "vocab_toy = build_vocab_from_iterator(map(tokenizer,iter(mytext)))\n",
        "\n",
        "# Print the encoded items\n",
        "print(vocab_toy.itos)\n",
        "# Print the dictionary\n",
        "print(vocab_toy.stoi)\n",
        "# Print all the numerical codes\n",
        "print([vocab_toy[w] for w in vocab_toy.itos])\n",
        "\n",
        "# Test with some items (unknown or known)\n",
        "print(vocab_toy['<unk>'])\n",
        "print(vocab_toy['<pad>'])\n",
        "print(vocab_toy['Polytech'])\n",
        "print(vocab_toy['the'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPO3GgI-Ql56"
      },
      "source": [
        "### Question: what is the role of the function \"data_process\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fkkyEL1Ql56"
      },
      "source": [
        "It transforms the raw text (english words) into a tensor of numbers.\n",
        "The raw text is decomposed into tokens with \"tokenizer\" defined above.\n",
        "The numerical encoding of a token is then obtained with \"vocab\" defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANbVqGvKiz_J"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter):\n",
        "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
        "                       dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
        "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
        "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKMHksyqQl57",
        "outputId": "e6387af8-634c-4b56-8529-4b63e0b4eb9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Item n°0\n",
            " \n",
            "\n",
            "----------------------------------------\n",
            "Item n°1\n",
            " = Valkyria Chronicles III = \n",
            "\n",
            "=\n",
            "valkyria\n",
            "chronicles\n",
            "iii\n",
            "=\n",
            "----------------------------------------\n",
            "Item n°2\n",
            " \n",
            "\n",
            "----------------------------------------\n",
            "Item n°3\n",
            " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
            "\n",
            "senjō\n",
            "no\n",
            "valkyria\n",
            "3\n",
            "<unk>\n",
            "chronicles\n",
            "(\n",
            "japanese\n",
            "戦場のヴァルキュリア3\n",
            ",\n",
            "lit\n",
            ".\n",
            "valkyria\n",
            "of\n",
            "the\n",
            "battlefield\n",
            "3\n",
            ")\n",
            ",\n",
            "commonly\n",
            "referred\n",
            "to\n",
            "as\n",
            "valkyria\n",
            "chronicles\n",
            "iii\n",
            "outside\n",
            "japan\n",
            ",\n",
            "is\n",
            "a\n",
            "tactical\n",
            "role\n",
            "@-@\n",
            "playing\n",
            "video\n",
            "game\n",
            "developed\n",
            "by\n",
            "sega\n",
            "and\n",
            "media\n",
            ".\n",
            "vision\n",
            "for\n",
            "the\n",
            "playstation\n",
            "portable\n",
            ".\n",
            "released\n",
            "in\n",
            "january\n",
            "2011\n",
            "in\n",
            "japan\n",
            ",\n",
            "it\n",
            "is\n",
            "the\n",
            "third\n",
            "game\n",
            "in\n",
            "the\n",
            "valkyria\n",
            "series\n",
            ".\n",
            "<unk>\n",
            "the\n",
            "same\n",
            "fusion\n",
            "of\n",
            "tactical\n",
            "and\n",
            "real\n",
            "@-@\n",
            "time\n",
            "gameplay\n",
            "as\n",
            "its\n",
            "predecessors\n",
            ",\n",
            "the\n",
            "story\n",
            "runs\n",
            "parallel\n",
            "to\n",
            "the\n",
            "first\n",
            "game\n",
            "and\n",
            "follows\n",
            "the\n",
            "nameless\n",
            ",\n",
            "a\n",
            "penal\n",
            "military\n",
            "unit\n",
            "serving\n",
            "the\n",
            "nation\n",
            "of\n",
            "gallia\n",
            "during\n",
            "the\n",
            "second\n",
            "europan\n",
            "war\n",
            "who\n",
            "perform\n",
            "secret\n",
            "black\n",
            "operations\n",
            "and\n",
            "are\n",
            "pitted\n",
            "against\n",
            "the\n",
            "imperial\n",
            "unit\n",
            "<unk>\n",
            "raven\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "raw_text_iter=iter(io.open(train_filepath, encoding=\"utf8\"))\n",
        "nb_item = 0\n",
        "for item in raw_text_iter:\n",
        "    print('-' * 40)\n",
        "    print(\"Item n°\"+str(nb_item))\n",
        "    print(item)\n",
        "    for token in tokenizer(item):\n",
        "        print(token)\n",
        "\n",
        "    # We just read 4 items (the four first lines)\n",
        "    if nb_item == 3:\n",
        "        break\n",
        "    else:\n",
        "        nb_item += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwqaE4KRQl57",
        "outputId": "659fef49-689c-4c90-c3ba-6ec2100c648f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([2049990])\n",
            "tensor([   10,  3850,  3870,   882,    10, 20001,    84,  3850,    89,     0])\n"
          ]
        }
      ],
      "source": [
        "print(type(train_data))\n",
        "# The length of train_data corresponds to the sequence of items that have been encoded.\n",
        "print(train_data.shape)\n",
        "# Print the 10 first first items as numbers\n",
        "print(train_data[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oozcpydQl57"
      },
      "source": [
        "We reduce the size of the dataset to get results faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDI56iYCQl57",
        "outputId": "a4af7923-37cf-40b2-df8c-15a4129414a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2049990])\n",
            "torch.Size([214417])\n",
            "torch.Size([241859])\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)\n",
        "train_data = train_data[0:10000]\n",
        "val_data = val_data[0:10000]\n",
        "test_data = test_data[0:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtDYy5DikZPm"
      },
      "source": [
        "Use a GPU if possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbxzg8VMkOlV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym8sbZC3kWAm"
      },
      "source": [
        "Create the batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDWYDcthQl58"
      },
      "source": [
        "### Question: what is the physical meaning (with respect to the raw text) of a batch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQHMudIcQl58"
      },
      "source": [
        "### Response:\n",
        "Each column corresponds to a batch.\n",
        "The successive elements of a column correspond to the successive elements of the input 1D tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHOqlrTvQl58",
        "outputId": "498a2164-2535-4f1d-e888-c0e8367fa6aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ],
      "source": [
        "### Response:\n",
        "#import numpy as np\n",
        "#A = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
        "# The following example shows how \"batchify\" is reordering a 1D tensor\n",
        "A = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "print(A)\n",
        "B = A.view(2, -1).t().contiguous()\n",
        "print(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHhzTJ2FkQM-"
      },
      "outputs": [],
      "source": [
        "def batchify(data, bsz):\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz # number of elements in a batch\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    # \"contiguous()\" returns a contiguous in memory tensor containing the same data as self tensor.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20 # number of batches\n",
        "eval_batch_size = 10\n",
        "\n",
        "import copy\n",
        "train_data_initial = copy.deepcopy(train_data)\n",
        "\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "louq-uRTQl58",
        "outputId": "ac550fb8-338c-4da6-9bf8-d052b69c7ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([500, 20])\n",
            "torch.Size([1000, 10])\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NPamPsiz_K"
      },
      "source": [
        "## Functions to generate input and target sequence\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VHdaLV1iz_K"
      },
      "source": [
        "``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/transformer_input_target.png)\n",
        "\n",
        "It means that from each column of \"Input\" (a column is a sequence), we want to predict the column of \"Target\".\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-EW7cwhQl59"
      },
      "source": [
        "### Question: what is returned by \"get_batch\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRd7WFwkQl59"
      },
      "source": [
        "### Response:\n",
        "\"get_batch\" returns a kind of minibatch: each \"data\" returned by this function corresponds to a certain number of rows of each batch. The number of colums is unchanged (the number of batch remains constant).\n",
        "\n",
        "The \"target\" is a minibatch with one row shift \"near the future\" compared to the \"data\" minibatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV8xDmYLiz_K"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoeiS1xKQl59",
        "outputId": "6144498d-6d2b-4cd4-bc4f-d2ee62a8f0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([35, 20])\n",
            "torch.Size([700])\n",
            "tensor([   10,  3850,  3870,   882,    10, 20001,    84,  3850,    89,     0,\n",
            "         3870,    22,   781, 28781,     3,  6183,     4,  3850,     5,     2,\n",
            "         5024,    89,    21,     3,  1838,  1019,     8,    15,  3850,  3870,\n",
            "          882,   630,   977,     3,    24])\n",
            "tensor([ 3850,  3870,   882,    10, 20001,    84,  3850,    89,     0,  3870,\n",
            "           22,   781, 28781,     3,  6183,     4,  3850,     5,     2,  5024,\n",
            "           89,    21,     3,  1838,  1019,     8,    15,  3850,  3870,   882,\n",
            "          630,   977,     3,    24,     9])\n",
            "torch.Size([35, 20])\n",
            "torch.Size([700])\n",
            "tensor([    9,  5791,   300,    13,   576,   233,    68,   453,    20, 13723,\n",
            "            6,   758,     4,  2501,    18,     2,  1768,  5638,     4,   156,\n",
            "            7,   247,   355,     7,   977,     3,    25,    24,     2,   238,\n",
            "           68,     7,     2,  3850,    94])\n",
            "tensor([ 5791,   300,    13,   576,   233,    68,   453,    20, 13723,     6,\n",
            "          758,     4,  2501,    18,     2,  1768,  5638,     4,   156,     7,\n",
            "          247,   355,     7,   977,     3,    25,    24,     2,   238,    68,\n",
            "            7,     2,  3850,    94,     4])\n",
            "torch.Size([35, 20])\n",
            "torch.Size([700])\n",
            "tensor([    4,     0,     2,   157,  4420,     5,  5791,     6,   730,    13,\n",
            "           59,  2097,    15,    44,  7076,     3,     2,   334,  1086,  3219,\n",
            "            8,     2,    38,    68,     6,  1695,     2, 11220,     3,     9,\n",
            "        19699,   314,  1064,  2083,     2])\n",
            "tensor([    0,     2,   157,  4420,     5,  5791,     6,   730,    13,    59,\n",
            "         2097,    15,    44,  7076,     3,     2,   334,  1086,  3219,     8,\n",
            "            2,    38,    68,     6,  1695,     2, 11220,     3,     9, 19699,\n",
            "          314,  1064,  2083,     2,  1703])\n"
          ]
        }
      ],
      "source": [
        "### Response:\n",
        "# Print 3 minibatches to show the link between the data and the targets\n",
        "for batch, i in enumerate(range(0, 3*bptt, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    print(data.shape)\n",
        "    print(targets.shape)\n",
        "    print(data[:,0])\n",
        "    print(targets.view(35, 20)[:,0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtYDQLfiz_L"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pknl3SUDiz_L"
      },
      "source": [
        "The model is set up with the hyperparameter below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OH8RSa6iz_L"
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension (it corresponds to \"ninp\")\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIbuuW6niz_M"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncoyLgh0iz_N"
      },
      "source": [
        "[``CrossEntropyLoss``](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) is applied to track the loss and\n",
        "[``SGD``](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD)\n",
        "implements stochastic gradient descent method as the optimizer.\n",
        "\n",
        "The initial learning rate is set to 5.0. [``StepLR``](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR) is\n",
        "applied to adjust the learn rate through epochs.\n",
        "\n",
        "During the\n",
        "training, we use\n",
        "[``nn.utils.clip_grad_norm_``](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_)\n",
        "function to scale all the gradient together to prevent exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIB8rvh5Ql5-"
      },
      "source": [
        "### Question: what is the role of \"StepLR\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTu-81iCQl5-"
      },
      "source": [
        "### Response:\n",
        "It decays the learning rate of each parameter group by gamma every step_size epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOu7E0KOhx39"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yslm_hqhQl5_"
      },
      "source": [
        "Perplexity is an evaluation criterion that has been well studied over the past few years\n",
        "\n",
        "Perplexity, called ppl in the next cell, is the exponentiation of the average cross entropy of a corpus (Mikolov et al., 2011)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj3UYVj9Ql5_"
      },
      "source": [
        "### Question: what is the role of \"torch.nn.utils.clip_grad_norm_\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e4uRgKjiz_N"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "log_interval = 20\n",
        "\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    # Loop over the training batches\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        #print(data.shape)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print a summary each log_interval iterations\n",
        "        # The summary is focused on the loss\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval # compute the current loss over the log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X23kltlQl5_"
      },
      "source": [
        "The next function evaluates a trained neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byPVmsBBhs6S"
      },
      "outputs": [],
      "source": [
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if data.size(0) != bptt:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xqhHcBiz_O"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "lDDZSKD_iz_O",
        "outputId": "e52b83d0-b87d-44f1-fc9b-2e4c280045d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 11.16s | valid loss  8.76 | valid ppl  6372.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 10.99s | valid loss  8.23 | valid ppl  3758.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 10.37s | valid loss  9.90 | valid ppl 19894.84\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train() # train a model for one epoch\n",
        "    val_loss = evaluate(model, val_data) # evaluate the performance of the trained model\n",
        "    # Print the performance of the trained model on the evaluation dataset\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # Keep the model if the loss decreases\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step() # Update the scheduler step with \"StepLR\" at each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6eQys0iz_O"
      },
      "source": [
        "Evaluate the model with the test dataset\n",
        "-------------------------------------\n",
        "\n",
        "Apply the best model to check the result with the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvrXlHNfiz_O",
        "outputId": "1d6b9ce7-9f57-44c5-c63e-63802220c7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  9.94 | test ppl 20711.95\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}